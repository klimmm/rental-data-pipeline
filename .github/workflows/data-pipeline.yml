name: Data Pipeline

on:
  schedule:
    # Run every hour for update/wide
    - cron: '0 * * * *'
    # Run daily at 3 AM for revise/wide (replacing the hourly update)
    - cron: '0 3 * * *'
  workflow_dispatch:
    inputs:
      update_current_search_listings:
        description: 'Update all current search listings'
        required: false
        default: false
        type: boolean
      check_if_unpublished:
        description: 'Check if listings are unpublished'
        required: false
        default: true
        type: boolean
      check_missing_estimations:
        description: 'Check listings with missing price estimations'
        required: false
        default: false
        type: boolean
      check_missing:
        description: 'Check missing listings'
        required: false
        default: false
        type: boolean
      update_unpublished_by_search:
        description: 'Update unpublished status based on search results'
        required: false
        default: false
        type: boolean
      search:
        description: 'Search config'
        required: false
        default: 'wide'
        type: choice
        options:
          - narrow
          - wide

env:
  PYTHON_VERSION: '3.11'

jobs:
  data-scraping:
    runs-on: ubuntu-latest
    timeout-minutes: ${{ (github.event.schedule == '0 3 * * *' || inputs.check_missing_estimations) && 120 || 60 }}  # 2 hours for daily run or when checking estimations, 1 hour otherwise
    
    steps:
    - name: Checkout pipeline repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        path: pipeline

    - name: Checkout main data repository
      uses: actions/checkout@v4
      with:
        repository: klimmm/cian-tracker
        token: ${{ secrets.PAT_TOKEN }}
        path: cian-tracker
        sparse-checkout: |
          data/cian_data
        sparse-checkout-cone-mode: false

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        cache-dependency-path: 'pipeline/.github/requirements-workflow-data-pipeline.txt'

    - name: Cache Playwright browsers
      uses: actions/cache@v4
      with:
        path: ~/.cache/ms-playwright
        key: playwright-${{ runner.os }}-${{ hashFiles('pipeline/.github/requirements-workflow-data-pipeline.txt') }}
        restore-keys: |
          playwright-${{ runner.os }}-

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y curl wget
        
    - name: Install Python dependencies and browsers
      run: |
        pip install -r pipeline/.github/requirements-workflow-data-pipeline.txt
        playwright install chromium --with-deps

    - name: Set up git configuration
      run: |
        git config --global user.name "github-actions[bot]"
        git config --global user.email "github-actions[bot]@users.noreply.github.com"

    - name: Install SingBox (VPN client)
      run: |
        if [ -f pipeline/.github/install-singbox.sh ]; then
          chmod +x pipeline/.github/install-singbox.sh
          ./pipeline/.github/install-singbox.sh
        else
          echo "SingBox install script not found, skipping"
        fi

    - name: Ensure data directory exists
      run: |
        mkdir -p cian-tracker/data/cian_data
        ls -la cian-tracker/data/cian_data/ || echo "Directory is empty"
        echo "Ensured data directory structure exists"

    - name: Run data scraping pipeline
      env:
        GITHUB_TOKEN: ${{ secrets.PAT_TOKEN }}
        BASE_URL: ${{ secrets.BASE_URL }}
        # Individual scraper parameters
        UPDATE_CURRENT_SEARCH_LISTINGS: ${{ inputs.update_current_search_listings || 'false' }}
        CHECK_IF_UNPUBLISHED: ${{ inputs.check_if_unpublished || (github.event.schedule && 'true') || 'false' }}
        CHECK_MISSING_ESTIMATIONS: ${{ inputs.check_missing_estimations || (github.event.schedule == '0 3 * * *' && 'true') || 'false' }}
        CHECK_MISSING: ${{ inputs.check_missing || 'false' }}
        UPDATE_UNPUBLISHED_BY_SEARCH: ${{ inputs.update_unpublished_by_search || 'false' }}
        SCRAPER_SEARCH: ${{ inputs.search || vars.SCRAPER_SEARCH || 'wide' }}
      run: |
        cd pipeline
        python -c "
        import asyncio
        import sys
        import os
        sys.path.append('.')
        from parse_data import ScraperPipeline
        
        # Get individual parameters from environment variables
        update_current_search_listings = os.environ.get('UPDATE_CURRENT_SEARCH_LISTINGS', 'false').lower() == 'true'
        check_if_unpublished = os.environ.get('CHECK_IF_UNPUBLISHED', 'true').lower() == 'true'
        check_missing_estimations = os.environ.get('CHECK_MISSING_ESTIMATIONS', 'false').lower() == 'true'
        check_missing = os.environ.get('CHECK_MISSING', 'false').lower() == 'true'
        update_unpublished_by_search = os.environ.get('UPDATE_UNPUBLISHED_BY_SEARCH', 'false').lower() == 'true'
        search = os.environ.get('SCRAPER_SEARCH', 'wide')
        
        print(f'🔧 Running with parameters:')
        print(f'  - update_current_search_listings: {update_current_search_listings}')
        print(f'  - check_if_unpublished: {check_if_unpublished}')
        print(f'  - check_missing_estimations: {check_missing_estimations}')
        print(f'  - check_missing: {check_missing}')
        print(f'  - update_unpublished_by_search: {update_unpublished_by_search}')
        print(f'  - search: {search}')
        
        # Set search config path
        if search == 'narrow':
            search_config_path = 'search_configs/search_narrow.yaml'
        elif search == 'wide':
            search_config_path = 'search_configs/search_wide.yaml'
        else:
            print(f'❌ Unknown search: {search}')
            sys.exit(1)
        
        async def main():
            try:
                pipeline = ScraperPipeline(
                    data_dir='../cian-tracker/data/cian_data',
                    use_proxies=False,
                    search_config_path=search_config_path,
                    check_missing_estimations=check_missing_estimations,
                    check_if_unpublished=check_if_unpublished,
                    update_current_search_listings=update_current_search_listings,
                    check_missing=check_missing,
                    update_unpublished_by_search=update_unpublished_by_search
                )
                await pipeline.run()
                print('✅ Scraper pipeline completed successfully')
                return True
            except Exception as e:
                print(f'❌ Scraper pipeline failed: {e}')
                import traceback
                traceback.print_exc()
                return False
        
        success = asyncio.run(main())
        sys.exit(0 if success else 1)
        "

    - name: Check for data changes
      id: changes
      run: |
        cd cian-tracker
        git add data/
        if git diff --cached --quiet; then
          echo "has_changes=false" >> $GITHUB_OUTPUT
          echo "ℹ️ No changes to commit"
        else
          echo "has_changes=true" >> $GITHUB_OUTPUT
          echo "📊 Changes detected:"
          git diff --cached --name-status
        fi

    - name: Commit and push changes
      if: steps.changes.outputs.has_changes == 'true'
      run: |
        cd cian-tracker
        
        # Get current timestamp
        timestamp=$(date '+%Y-%m-%d %H:%M:%S')
        
        # Determine trigger type and parameters
        if [ "${{ github.event.schedule }}" = "0 3 * * *" ]; then
          schedule_type="(daily run with estimations check)"
        elif [ "${{ github.event.schedule }}" = "0 * * * *" ]; then
          schedule_type="(hourly update)"
        else
          # Manual trigger - show parameters
          params=""
          [ "${{ inputs.update_current_search_listings }}" = "true" ] && params="${params} update_all"
          [ "${{ inputs.check_if_unpublished }}" = "true" ] && params="${params} check_unpublished"
          [ "${{ inputs.check_missing_estimations }}" = "true" ] && params="${params} check_estimations"
          [ "${{ inputs.check_missing }}" = "true" ] && params="${params} check_missing"
          [ "${{ inputs.update_unpublished_by_search }}" = "true" ] && params="${params} update_unpublished"
          [ -z "$params" ] && params=" default"
          schedule_type="(manual:${params})"
        fi
        
        # Create commit message
        commit_msg="Auto-update data on $timestamp $schedule_type

        🤖 Generated with [Claude Code](https://claude.ai/code)

        Co-Authored-By: Claude <noreply@anthropic.com>"
        
        # Commit changes
        git commit -m "$commit_msg"
        
        # Handle potential remote changes
        echo "Fetching latest changes..."
        git fetch origin main
        
        # Stash any unstaged changes before rebase
        if ! git diff --quiet; then
          echo "Stashing unstaged changes..."
          git stash push -m "WIP: unstaged changes before rebase"
          stashed=true
        else
          stashed=false
        fi
        
        # Check if we need to rebase
        if ! git merge-base --is-ancestor origin/main HEAD; then
          echo "Remote has changes, rebasing..."
          git rebase origin/main || {
            echo "Rebase conflicts detected, using merge strategy instead..."
            git rebase --abort
            git merge origin/main -X ours -m "Merge remote changes (keeping local data updates)"
          }
        fi
        
        # Restore stashed changes if any
        if [ "$stashed" = "true" ]; then
          echo "Restoring stashed changes..."
          git stash pop || echo "No changes to restore"
        fi
        
        # Push with retry logic
        max_retries=3
        for i in $(seq 1 $max_retries); do
          if git push origin main; then
            echo "✅ Changes pushed successfully to cian-tracker"
            break
          else
            if [ $i -eq $max_retries ]; then
              echo "❌ Failed to push after $max_retries attempts"
              exit 1
            fi
            echo "⚠️ Push failed, retrying in 5 seconds... (attempt $i/$max_retries)"
            sleep 5
          fi
        done

    - name: Trigger image processing workflow
      if: steps.changes.outputs.has_changes == 'true' && (inputs.search || vars.SCRAPER_SEARCH || 'wide') == 'wide' && (!inputs.check_missing_estimations || 'false') == 'false'
      env:
        GITHUB_TOKEN: ${{ secrets.PAT_TOKEN }}
      run: |
        # Wait a moment for the data to be available
        sleep 10
        
        # Trigger the image processing workflow in image-utils submodule
        if gh workflow run process-images.yml --repo klimmm/image-utils; then
          echo "✅ Image processing workflow triggered successfully"
        else
          echo "⚠️ Could not trigger image processing workflow"
          # Don't fail the job - this is not critical
        fi

    - name: Upload logs on failure
      if: failure()
      uses: actions/upload-artifact@v4
      with:
        name: pipeline-logs-${{ github.run_number }}
        path: |
          *.log
          logs/
        retention-days: 7

  health-check:
    runs-on: ubuntu-latest
    needs: data-scraping
    if: always()
    
    steps:
    - name: Report pipeline status
      run: |
        if [ "${{ needs.data-scraping.result }}" = "success" ]; then
          echo "✅ Data pipeline completed successfully"
        else
          echo "❌ Data pipeline failed"
          echo "Job result: ${{ needs.data-scraping.result }}"
        fi
        
        # You could add additional health checks here:
        # - Check data freshness
        # - Validate data integrity
        # - Send notifications to external services